{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c2d2382-9e9b-435d-aaea-f3d913a1ca81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n",
      "Clean data file: C:\\Users\\jampa\\Videos\\Ghost Type Corrector\\ai_model\\data\\train_clean.txt\n",
      "Noisy data file: C:\\Users\\jampa\\Videos\\Ghost Type Corrector\\ai_model\\data\\train_noisy.txt\n",
      "\n",
      "--- Previewing first few lines ---\n",
      "Clean Sample 1: waiting time tariff each period of one minute or part\n",
      "Noisy Sample 1: waiting time tariuff each period of one minut or part\n",
      "\n",
      "Clean Sample 2: christian catholic relabelled as christian roman catholic for consistency with census labels\n",
      "Noisy Sample 2: christian catholic relabelled as christian rman catholic for cfnsistency with census labels\n",
      "\n",
      "Clean Sample 3: per night per person\n",
      "Noisy Sample 3: per night per person\n",
      "\n",
      "Clean Sample 4: mile south of the town which is included in the heritage walk vale trail\n",
      "Noisy Sample 4: mile south of the town which is includeyd in the heritage walk ale trial\n",
      "\n",
      "Clean Sample 5: mile you will see the entrance to t cerrig woodland retreats on your left\n",
      "Noisy Sample 5: mile you wull see the entrance to t jcerrig woodland retreats on your left\n",
      "\n",
      "\n",
      "Successfully loaded 5 sample lines.\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# CELL 1: IMPORTS AND DATA LOADING SETUP\n",
    "###\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "# --- Define File Paths ---\n",
    "# We are in 'notebooks', go up one ('..') to 'ai_model'\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "clean_file_path = os.path.join(data_dir, 'train_clean.txt')\n",
    "noisy_file_path = os.path.join(data_dir, 'train_noisy.txt')\n",
    "\n",
    "print(f\"Clean data file: {clean_file_path}\")\n",
    "print(f\"Noisy data file: {noisy_file_path}\")\n",
    "\n",
    "# --- Load a Small Sample to Verify ---\n",
    "num_samples_to_preview = 5\n",
    "clean_lines_sample = []\n",
    "noisy_lines_sample = []\n",
    "\n",
    "try:\n",
    "    with open(clean_file_path, 'r', encoding='utf-8') as f_clean, \\\n",
    "         open(noisy_file_path, 'r', encoding='utf-8') as f_noisy:\n",
    "\n",
    "        print(\"\\n--- Previewing first few lines ---\")\n",
    "        for i in range(num_samples_to_preview):\n",
    "            clean_line = f_clean.readline().strip()\n",
    "            noisy_line = f_noisy.readline().strip()\n",
    "            if not clean_line or not noisy_line:\n",
    "                break\n",
    "            clean_lines_sample.append(clean_line)\n",
    "            noisy_lines_sample.append(noisy_line)\n",
    "            print(f\"Clean Sample {i+1}: {clean_line}\")\n",
    "            print(f\"Noisy Sample {i+1}: {noisy_line}\\n\")\n",
    "\n",
    "    print(f\"\\nSuccessfully loaded {len(clean_lines_sample)} sample lines.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Could not find training files in {data_dir}\")\n",
    "    print(\"Please make sure 'train_clean.txt' and 'train_noisy.txt' exist.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "061fed8a-3956-43f0-bc0e-30c1180086e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Preparation ---\n",
      "Loading 100000 lines from files...\n",
      "Loaded 42985 pairs of lines.\n",
      "\n",
      "Vocabulary Size: 30\n",
      "Sample vocabulary mapping: [('', 0), ('\\t', 1), ('\\n', 2), (' ', 3), ('a', 4), ('b', 5), ('c', 6), ('d', 7), ('e', 8), ('f', 9)]...\n",
      "\n",
      "Vectorizing text...\n",
      "Max sequence length (including START/END tokens): 101\n",
      "Padding sequences...\n",
      "\n",
      "--- Data Preparation Complete ---\n",
      "Shape of noisy_padded (Input X): (42985, 101)\n",
      "Shape of clean_padded (Target Y): (42985, 101)\n",
      "\n",
      "Saved tokenizer configuration to: C:\\Users\\jampa\\Videos\\Ghost Type Corrector\\ai_model\\data\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# CELL 2: TOKENIZATION AND DATA PREPARATION\n",
    "###\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json # To save our tokenizer configuration\n",
    "\n",
    "print(\"--- Starting Data Preparation ---\")\n",
    "\n",
    "# --- Reload data (adjust NUM_LINES if needed for faster testing) ---\n",
    "# Set NUM_LINES to None to load ALL lines (will take longer)\n",
    "NUM_LINES = 100000 # Let's start with 100k lines for faster processing initially\n",
    "# NUM_LINES = None # Uncomment this to use the full dataset later\n",
    "\n",
    "print(f\"Loading {NUM_LINES if NUM_LINES else 'all'} lines from files...\")\n",
    "\n",
    "clean_lines = []\n",
    "noisy_lines = []\n",
    "try:\n",
    "    with open(clean_file_path, 'r', encoding='utf-8') as f_clean, \\\n",
    "         open(noisy_file_path, 'r', encoding='utf-8') as f_noisy:\n",
    "        \n",
    "        line_num = 0\n",
    "        while True:\n",
    "            clean_line = f_clean.readline().strip()\n",
    "            noisy_line = f_noisy.readline().strip()\n",
    "            \n",
    "            if not clean_line or not noisy_line:\n",
    "                break # End of file\n",
    "            \n",
    "            # Simple filter: skip very long lines which might be noise or headers\n",
    "            if len(clean_line) < 100 and len(noisy_line) < 100:\n",
    "                clean_lines.append(clean_line)\n",
    "                noisy_lines.append(noisy_line)\n",
    "                \n",
    "            line_num += 1\n",
    "            if NUM_LINES is not None and line_num >= NUM_LINES:\n",
    "                break # Stop after reaching NUM_LINES\n",
    "\n",
    "    print(f\"Loaded {len(clean_lines)} pairs of lines.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    # Stop execution if data loading fails\n",
    "    raise\n",
    "\n",
    "# --- Character Tokenization ---\n",
    "# We treat the problem as character-level seq2seq\n",
    "\n",
    "# 1. Build Vocabulary\n",
    "# We need START, END, and PADDING tokens in addition to our alphabet\n",
    "START_TOKEN = '\\t' # Indicates start of sequence (often used in seq2seq)\n",
    "END_TOKEN = '\\n'   # Indicates end of sequence\n",
    "PAD_TOKEN = ''     # Represents padding (Keras handles index 0 automatically)\n",
    "\n",
    "# Find all unique characters in both clean and noisy text\n",
    "all_text = \" \".join(clean_lines + noisy_lines)\n",
    "chars = sorted(list(set(all_text)))\n",
    "vocabulary = [PAD_TOKEN, START_TOKEN, END_TOKEN] + chars # Ensure PAD=0, START=1, END=2\n",
    "char_to_index = {char: index for index, char in enumerate(vocabulary)}\n",
    "index_to_char = {index: char for index, char in enumerate(vocabulary)}\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "print(f\"\\nVocabulary Size: {vocab_size}\")\n",
    "print(f\"Sample vocabulary mapping: {list(char_to_index.items())[:10]}...\") # Show first 10 mappings\n",
    "\n",
    "# --- Vectorization and Padding ---\n",
    "# We need to convert sentences to sequences of indices\n",
    "\n",
    "def vectorize_text(text_list):\n",
    "    vectorized = []\n",
    "    for text in text_list:\n",
    "        # Add START and END tokens\n",
    "        tokens = [char_to_index[START_TOKEN]] + [char_to_index[char] for char in text] + [char_to_index[END_TOKEN]]\n",
    "        vectorized.append(tokens)\n",
    "    return vectorized\n",
    "\n",
    "print(\"\\nVectorizing text...\")\n",
    "noisy_vectors = vectorize_text(noisy_lines)\n",
    "clean_vectors = vectorize_text(clean_lines)\n",
    "\n",
    "# Find the maximum length needed for padding\n",
    "max_len_noisy = max(len(vec) for vec in noisy_vectors)\n",
    "max_len_clean = max(len(vec) for vec in clean_vectors)\n",
    "max_seq_length = max(max_len_noisy, max_len_clean)\n",
    "\n",
    "print(f\"Max sequence length (including START/END tokens): {max_seq_length}\")\n",
    "\n",
    "# Pad sequences\n",
    "# 'post' means add padding at the end\n",
    "print(\"Padding sequences...\")\n",
    "noisy_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    noisy_vectors, maxlen=max_seq_length, padding='post'\n",
    ")\n",
    "clean_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    clean_vectors, maxlen=max_seq_length, padding='post'\n",
    ")\n",
    "\n",
    "print(\"\\n--- Data Preparation Complete ---\")\n",
    "print(f\"Shape of noisy_padded (Input X): {noisy_padded.shape}\")   # Should be (NUM_LINES, max_seq_length)\n",
    "print(f\"Shape of clean_padded (Target Y): {clean_padded.shape}\") # Should be (NUM_LINES, max_seq_length)\n",
    "\n",
    "# --- Save Tokenizer Config ---\n",
    "# We need to save char_to_index and max_seq_length to use them later\n",
    "# during inference (in the extension) and for the conversion script.\n",
    "tokenizer_config = {\n",
    "    'char_to_index': char_to_index,\n",
    "    'index_to_char': index_to_char,\n",
    "    'max_seq_length': max_seq_length,\n",
    "    'vocab_size': vocab_size,\n",
    "    'start_token_index': char_to_index[START_TOKEN],\n",
    "    'end_token_index': char_to_index[END_TOKEN],\n",
    "    'pad_token_index': char_to_index[PAD_TOKEN] # Should be 0\n",
    "}\n",
    "\n",
    "config_save_path = os.path.join(data_dir, 'tokenizer_config.json')\n",
    "try:\n",
    "    with open(config_save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tokenizer_config, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\nSaved tokenizer configuration to: {config_save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving tokenizer config: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f36deb-a5aa-486c-9524-faf7a134b97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Defining Model Architecture ---\n",
      "\n",
      "--- Model Architecture Defined ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2seq_autocorrect\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"seq2seq_autocorrect\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoder_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)           │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ encoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]        │                 │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)           │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, │         <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ decoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]        │                 │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],        │\n",
       "│                               │                           │                 │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ output_dense                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">7,710</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)             │                           │                 │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m101\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder_input (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m101\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoder_embedding (\u001b[38;5;33mEmbedding\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m101\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │           \u001b[38;5;34m3,840\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m101\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder_embedding (\u001b[38;5;33mEmbedding\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m101\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │           \u001b[38;5;34m3,840\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)           │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │         \u001b[38;5;34m394,240\u001b[0m │ encoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                               │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]        │                 │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)           │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m101\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, │         \u001b[38;5;34m394,240\u001b[0m │ decoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                               │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]        │                 │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],        │\n",
       "│                               │                           │                 │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal_1 (\u001b[38;5;33mNotEqual\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m101\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ output_dense                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m101\u001b[0m, \u001b[38;5;34m30\u001b[0m)           │           \u001b[38;5;34m7,710\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)             │                           │                 │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">803,870</span> (3.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m803,870\u001b[0m (3.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">803,870</span> (3.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m803,870\u001b[0m (3.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###\n",
    "# CELL 3: DEFINE SEQ2SEQ MODEL ARCHITECTURE\n",
    "###\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, TimeDistributed\n",
    "\n",
    "print(\"--- Defining Model Architecture ---\")\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "# These are settings we can tune later to improve the model\n",
    "embedding_dim = 128  # Size of the vector for each character\n",
    "latent_dim = 256     # Number of units in the LSTM layers (complexity)\n",
    "\n",
    "# --- Encoder ---\n",
    "# Takes the noisy sequence as input\n",
    "encoder_inputs = Input(shape=(max_seq_length,), name='encoder_input') # max_seq_length comes from CELL 2\n",
    "\n",
    "# Embedding layer: Turns character indices into dense vectors\n",
    "# mask_zero=True tells LSTMs to ignore padding (0s)\n",
    "encoder_embedding = Embedding(input_dim=vocab_size, # vocab_size comes from CELL 2\n",
    "                              output_dim=embedding_dim,\n",
    "                              mask_zero=True,\n",
    "                              name='encoder_embedding')(encoder_inputs)\n",
    "\n",
    "# LSTM layer: Processes the sequence and outputs its final state\n",
    "# return_state=True gives us the hidden state (h) and cell state (c)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
    "# Using LSTM involves recurrent connections\n",
    "# \n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# We discard encoder_outputs, keeping only the states (the \"thought vector\")\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# --- Decoder ---\n",
    "# Takes the *clean* sequence as input during training (teacher forcing)\n",
    "# Note: Shape is (max_seq_length,) because we will shift it later\n",
    "decoder_inputs = Input(shape=(max_seq_length,), name='decoder_input') # max_seq_length comes from CELL 2\n",
    "\n",
    "# Embedding layer for the decoder (can use a separate one or reuse encoder's)\n",
    "decoder_embedding_layer = Embedding(input_dim=vocab_size, # vocab_size comes from CELL 2\n",
    "                                    output_dim=embedding_dim,\n",
    "                                    mask_zero=True,\n",
    "                                    name='decoder_embedding')\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "# Decoder LSTM:\n",
    "# return_sequences=True makes it output at *each* timestep\n",
    "# We initialize its state with the encoder's final state (encoder_states)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "# We discard the decoder's final states during training\n",
    "\n",
    "# --- Output Layer ---\n",
    "# TimeDistributed applies a Dense layer to *each* timestep of the decoder output\n",
    "# It predicts the probability of each character in our vocabulary using softmax\n",
    "# \n",
    "decoder_dense = TimeDistributed(Dense(vocab_size, activation='softmax'), name='output_dense') # vocab_size from CELL 2\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# --- Define the Model ---\n",
    "# Connects the encoder input, decoder input, and the final decoder output\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name='seq2seq_autocorrect')\n",
    "\n",
    "print(\"\\n--- Model Architecture Defined ---\")\n",
    "model.summary() # Print a summary of the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d9c927c-50e5-4ecd-960c-0b662a96fd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing Decoder Targets and Compiling Model ---\n",
      "\n",
      "Shape of noisy_padded (Encoder Input): (42985, 101)\n",
      "Shape of clean_padded (Decoder Input): (42985, 101)\n",
      "Shape of decoder_target_data (Decoder Target): (42985, 101)\n",
      "\n",
      "--- Model Compiled Successfully ---\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# CELL 4: PREPARE TARGETS AND COMPILE MODEL\n",
    "###\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Preparing Decoder Targets and Compiling Model ---\")\n",
    "\n",
    "# --- Prepare Decoder Target Data ---\n",
    "# The decoder target sequence should be the clean sequence shifted one step to the left\n",
    "# Example: If clean_padded is [START, c, a, t, END, PAD],\n",
    "#          decoder_target should be [c, a, t, END, PAD, PAD]\n",
    "\n",
    "# Create decoder_target_data by slicing clean_padded from the second element onwards\n",
    "decoder_target_data = clean_padded[:, 1:] # Shape: (num_samples, max_seq_length - 1)\n",
    "\n",
    "# We need to add one more padding step at the end to make its length equal to max_seq_length\n",
    "# Create a zero array with shape (num_samples, 1)\n",
    "padding_column = np.zeros((decoder_target_data.shape[0], 1), dtype=np.int32)\n",
    "\n",
    "# Concatenate the padding column to the end\n",
    "decoder_target_data = np.concatenate([decoder_target_data, padding_column], axis=-1)\n",
    "\n",
    "# One-Hot Encode the target data (required for sparse_categorical_crossentropy if not used)\n",
    "# However, sparse_categorical_crossentropy is more memory efficient as it works directly with indices.\n",
    "# We will use sparse_categorical_crossentropy, so we don't need to one-hot encode.\n",
    "# If we were using categorical_crossentropy, we would uncomment the line below:\n",
    "# decoder_target_one_hot = tf.keras.utils.to_categorical(decoder_target_data, num_classes=vocab_size)\n",
    "\n",
    "print(f\"\\nShape of noisy_padded (Encoder Input): {noisy_padded.shape}\")\n",
    "print(f\"Shape of clean_padded (Decoder Input): {clean_padded.shape}\")\n",
    "print(f\"Shape of decoder_target_data (Decoder Target): {decoder_target_data.shape}\")\n",
    "\n",
    "# --- Compile the Model ---\n",
    "# sparse_categorical_crossentropy works directly with integer indices (like ours)\n",
    "# adam is a standard, effective optimizer\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"\\n--- Model Compiled Successfully ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a88f42-032a-4990-ac40-6315390542ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Model Training ---\n",
      "Epoch 1/5\n",
      "\u001b[1m538/538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 507ms/step - accuracy: 0.2679 - loss: 2.0398 - val_accuracy: 0.3113 - val_loss: 1.8541\n",
      "Epoch 2/5\n",
      "\u001b[1m538/538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 517ms/step - accuracy: 0.3467 - loss: 1.6159 - val_accuracy: 0.3560 - val_loss: 1.6743\n",
      "Epoch 3/5\n",
      "\u001b[1m538/538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 534ms/step - accuracy: 0.3769 - loss: 1.4762 - val_accuracy: 0.3757 - val_loss: 1.5942\n",
      "Epoch 4/5\n",
      "\u001b[1m538/538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 524ms/step - accuracy: 0.3919 - loss: 1.4030 - val_accuracy: 0.3818 - val_loss: 1.5515\n",
      "Epoch 5/5\n",
      "\u001b[1m538/538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 512ms/step - accuracy: 0.4039 - loss: 1.3446 - val_accuracy: 0.3935 - val_loss: 1.5099\n",
      "\n",
      "--- Model Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# CELL 5: TRAIN THE MODEL\n",
    "###\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"--- Starting Model Training ---\")\n",
    "\n",
    "# --- Training Parameters ---\n",
    "epochs = 5  # Start with a small number for testing\n",
    "batch_size = 64\n",
    "\n",
    "# --- Prepare Inputs and Targets for model.fit ---\n",
    "# Encoder input: noisy sequences\n",
    "encoder_input_data = noisy_padded\n",
    "\n",
    "# Decoder input: clean sequences (used for teacher forcing)\n",
    "decoder_input_data = clean_padded\n",
    "\n",
    "# Decoder target: clean sequences shifted left by one step\n",
    "# (We already prepared this in CELL 4 as decoder_target_data)\n",
    "\n",
    "# --- Train the Model ---\n",
    "# This is where the learning happens!\n",
    "# The model will try to minimize the loss (sparse_categorical_crossentropy)\n",
    "# by adjusting its internal weights based on the input data and expected targets.\n",
    "# \n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2) # Use 20% of data for validation\n",
    "\n",
    "print(\"\\n--- Model Training Complete ---\")\n",
    "\n",
    "# --- Optional: Plot training history (requires matplotlib) ---\n",
    "# You can uncomment this section later if you install matplotlib (`pip install matplotlib`)\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# print(\"\\n--- Plotting Training History ---\")\n",
    "# plt.figure(figsize=(12, 4))\n",
    "#\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('Loss Over Epochs')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "#\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.title('Accuracy Over Epochs')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b454345c-e467-407e-8283-1032ce3ea9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Saving the Trained Model ---\n",
      "\n",
      "Model successfully saved to: C:\\Users\\jampa\\Videos\\Ghost Type Corrector\\ai_model\\autocorrect_model.h5\n",
      "File size: 9.245223999023438 MB\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# CELL 6: SAVE THE TRAINED MODEL\n",
    "###\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"--- Saving the Trained Model ---\")\n",
    "\n",
    "# Define the path where the model should be saved\n",
    "# We want to save it inside the 'ai_model' folder, not 'notebooks'\n",
    "model_save_path = os.path.join(base_dir, 'autocorrect_model.h5') # base_dir was defined in CELL 1\n",
    "\n",
    "try:\n",
    "    # Save the entire model (architecture + weights + optimizer state)\n",
    "    model.save(model_save_path)\n",
    "    print(f\"\\nModel successfully saved to: {model_save_path}\")\n",
    "    print(\"File size:\", os.path.getsize(model_save_path) / (1024 * 1024), \"MB\") # Print size in MB\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47f692-4c89-4a42-b218-0a88f89bb542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
